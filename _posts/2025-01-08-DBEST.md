---
title: "[Paper Reivew] On Manipulating Scene Text in the Wild with Diffusion Models"
description: One-shot style adaptation, Text-recognition guidance를 이용한 Diffusion based scene text editing을 수행하는 DBEST 제안.
toc: true
comments: true
# layout: default
math: true
date: 2025-01-08 18:59:00 +09:00
categories: [Deep Learning, Generative Model]
tags: [diffusion model, generative model, ste]     # TAG names should always be lowercase
image: /posts/20250108_DBEST/teaser.jpeg
alt : Thumbnail
author: Daemin
---

> WACV 2024 [[Paper](https://arxiv.org/abs/2311.00734)] <br/>
> Joshua Santoso, Christian Simon, Williem <br/>
> The Australian National University <br/>
> 3 Nov, 2023 <br/>


# TL;DR 
**One-shot style adaptation, Text-recognition guidance**를 이용한 Diffusion based scene text editing을 수행하는 DBEST 제안.

- 퀄리티는 그럭저럭
- Inference time이 ~8 min 정도로 사실상 사용 불가능한 방법론.
- 조작할 수 있는 character length에 제한이 있음.


## 1. Introduction

GAN 기반 Scene Text Editing의 성능은 좋지 않으며, T2I Diffusion model의 성능은 powerful 하지만, naive하게 STE에 적용하기에는 문제가 있다고 합니다. 저자들은 pre-trained diffusion model (LDM)을 base로 적절히 fine-tuning하여 STE에 적용하려는 시도를 했습니다.

![fig1](/posts/20250108_DBEST/fig1.png){: width="800" height="300"}

저자들의 주요 contribution은 다음과 같습니다. 

- 기존 GAN 기반에서 벗어나, Diffusion 기반의 STE을 하려는 시도.
- **One-shot style adaptation**으로 수정되는 이미지에서 source 이미지의 style을 유지.
- Text recognition model을 이용해 **guidance** 방법론 제안.


## 2. Related works
(생략)

## 3. Method

저자들은 conditional LDM 기반으로 방법론을 제안했는데, 크게 2 stage로 구성됩니다. 

1. **Fine-tune** on the the text and edited-text pairs. 
2. Two optimization processes (**one-shot style adaptation**, **text recognition guidance**)

### 3.1. One-shot style adaptation

![fig2a](/posts/20250108_DBEST/fig2a.png){: width="800" height="300"}

One-shot style adaptation에서 저자들은 source image를 이용해 pre-trained diffusion model을 fine-tuning합니다. 

$$
\min_\theta \| \epsilon - \epsilon_\theta(z_t, t, e_{\text{src}}) \|_2^2
$$

저자들의 방법론과 기존 방법의 차이는... 

1.DreamBooth에서는 unique identifier 사용하지만 DBEST에서는 사용하지 않는다고 합니다.
2. Textual Inversion에서는 detail loss가 자주 생긴다고 합니다. 

게다가 위 방법은 여러 장의 이미지를 필요로 하지만, One-shot style adaptation은 1장의 이미지만을 필요로 한다고 합니다.

### 3.2. Text recognition guidance

![fig2b](/posts/20250108_DBEST/fig2b.png){: width="800" height="300"}

위에서 각 source image에 대해 fine-tuned 모델$$ \tilde{\epsilon} $$이 있다고 하면, 저자들은 text recognizer를 이용해 $$ e_{\text{tgt}} $$ 를 optimize.


## 4. Experiments

### 4.1. Evaluations

![tab1](/posts/20250108_DBEST/tab1.png){: width="800" height="300"}


![fig3](/posts/20250108_DBEST/fig3.png){: width="800" height="300"}

![fig4](/posts/20250108_DBEST/fig4.png){: width="800" height="300"}


저자들의 **DBEST**가 가장 좋은 성능이라고 주장. 

### 4.2. Ablation studies

![tab3](/posts/20250108_DBEST/tab3.png){: width="600" height="300"}


- **Pre-trained with SynText dataset**

일반적인 LDM은 scene text domain을 다루는 능력이 없기 때문에 pre-train이 필요하다고 합니다.


- **One-shot style adaptation**

![fig6](/posts/20250108_DBEST/fig6.png){: width="800" height="300"}

Edited text에서 text geometry와 source의 외형을 유지하는데 중요한 역할을 한다고 합니다. 

- **Text recognition guidance**

![fig7](/posts/20250108_DBEST/fig7.png){: width="600" height="300"}

Text recognition guidance로 Diffusion 모델이 문자 오류를 수정하도록 유도하며, 단어 정확도가 크게 개선되었다고 합니다.

- **Length of characters**
길이가 5를 초과할 때 성능이 크게 감소했는데, 이는 주어진 텍스트와 장면 내 사용 가능한 공간에 따라 **폰트 크기를 조정할 능력 부족** 때문으로 판단된다고 합니다.

![fig9](/posts/20250108_DBEST/fig9.png){: width="500" height="300"}

